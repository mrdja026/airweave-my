search_defaults:
  offset: 0
  limit: 1000

  retrieval_strategy: hybrid

  temporal_relevance: 0.3

  expand_query: true
  interpret_filters: false
  rerank: true
  generate_answer: true

# Model specifications per provider
# Defines available models and their properties
provider_models:
  local:
    embedding:
      name: "local-text2vec-transformers"
      tokenizer: "cl100k_base"  # unused, placeholder for schema consistency
      dimensions: 384
      max_tokens: 8191

  ollama:
    llm:
      name: "gemma:7b"
      tokenizer: "cl100k_base"  # approximate for budgeting
      context_window: 8192
  openai:
    llm_big:
      name: "gpt-5"
      tokenizer: "cl100k_base"
      context_window: 400000
    llm_small:
      name: "gpt-5-nano"
      tokenizer: "cl100k_base"
      context_window: 400000
    embedding:
      name: "text-embedding-3-small"
      tokenizer: "cl100k_base"
      dimensions: 1536
      max_tokens: 8191
    rerank:
      name: "gpt-5-nano"
      tokenizer: "cl100k_base"
      context_window: 400000

  groq:
    llm_big:
      name: "openai/gpt-oss-120b"
      tokenizer: "o200k_harmony"
      context_window: 128000
    llm_small:
      name: "openai/gpt-oss-20b"
      tokenizer: "o200k_harmony"
      context_window: 128000
    embedding: null
    rerank:
      name: "openai/gpt-oss-120b"
      tokenizer: "o200k_harmony"
      context_window: 128000

  cerebras:
    llm:
      name: "gpt-oss-120b"
      tokenizer: "o200k_harmony"
      context_window: 131000
    embedding: null
    rerank: null

  cohere:
    llm: null
    embedding: null
    rerank:
      name: "rerank-v3.5"
      tokenizer: "cl100k_base"
      max_tokens_per_doc: 8191
      max_documents: 1000

# Operation preferences - which provider and which models to use
# Format: provider: {llm: model_key, embedding: model_key, rerank: model_key}
operation_preferences:
  query_expansion:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null

  query_interpretation:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null

  embed_query:
    order:
      - provider: local
        llm: null
        embedding: embedding
        rerank: null
      - provider: openai
        llm: null
        embedding: embedding
        rerank: null

  reranking:
    order:
      - provider: cohere
        llm: null
        embedding: null
        rerank: rerank
      - provider: groq
        llm: null
        embedding: null
        rerank: rerank
      - provider: openai
        llm: null
        embedding: null
        rerank: rerank

  generate_answer:
    order:
      - provider: ollama
        llm: llm
        embedding: null
        rerank: null
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null

  federated_search:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null
